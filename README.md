# torchrl_mcts
This is a proof of concept on how MCTS can be implemented on top of TorchRL

# Resources

1. [A Simple Alpha(Go) Zero Tutorial - Stanford](https://web.stanford.edu/~surag/posts/alphazero.html)
2. [Stanford CS234: Reinforcement Learning | Winter 2019 | Lecture 16 - Monte Carlo Tree Search](https://www.youtube.com/watch?v=vDF1BYWhqL8)
3. [Monte-Carlo Tree Search (MCTS) - Tim Miller](https://gibberblot.github.io/rl-notes/single-agent/mcts.html#)
4. [Alpha Zero General - Github](https://github.com/suragnair/alpha-zero-general)
5. [David Silver: Simulation-Based Search - Youtube](https://www.youtube.com/watch?v=SzosiqyjpHE&t=1058s)
6. [Silver, David, Richard S. Sutton, and Martin MÃ¼ller. "Temporal-difference search in computer Go."](https://www.davidsilver.uk/wp-content/uploads/2020/03/tdsearch_compressed.pdf)
7. AlphaGo Papers:
   1. [Mastering the game of Go with deep neural networks and tree search](https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf)
8. AlphaZero Papers: 
   1. [A general reinforcement learning algorithm that masters chess, shogi and Go through self-play](https://discovery.ucl.ac.uk/id/eprint/10069050/1/alphazero_preprint.pdf)
   2. [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815.pdf)
9. MuZero Papers: 
   1. [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/pdf/1911.08265.pdf&lang=en)

